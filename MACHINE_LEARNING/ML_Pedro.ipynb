{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Factores controlan la intensidad de un sismos a grandes razgos:\n",
    "- Magnitud\n",
    "- Distancia del hipocentro\n",
    "- Suelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak acceleration\n",
    "- How quickly the ground shook\n",
    "- How swiftly it changed direction\n",
    "- Built on mud substrate\n",
    "  - Bay Mud\n",
    "  - Allavium\n",
    "  - Bedrock\n",
    "Documentr that seismic waves are amplified by a facto of two as they pass from bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw_usa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"properties.time\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cdi y mmi, analizar que hacer con esos pocos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['properties.cdi',\t'properties.mmi'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(['type', 'id', 'properties.place', 'properties.time',\n",
    "       'properties.updated', 'properties.tz', 'properties.url',\n",
    "       'properties.detail', 'properties.felt', 'properties.status',\n",
    "       'properties.tsunami', 'properties.net', \"properties.magType\",\n",
    "       'properties.code', 'properties.ids', 'properties.sources',\n",
    "       'properties.nst', 'properties.dmin','properties.title',\n",
    "         'geometry.type', 'geometry.coordinates', 'properties.rms',\n",
    "       'Longitud', 'Latitud', 'properties.types'], axis = 1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un terremoto no puede producirse físicamente a una profundidad de 0 km o -1 km (por encima de la superficie de la Tierra). Para que se produzca un terremoto, dos bloques de corteza deben deslizarse entre sí, y es imposible que esto ocurra en la superficie terrestre o por encima de ella. Entonces, ¿por qué a veces informamos de que el seísmo se produjo a una profundidad de 0 km o de un suceso como una profundidad negativa?\n",
    "\n",
    "En primer lugar, la profundidad de un terremoto suele ser la parte más difícil de precisar. Dado que la mayoría de los terremotos se producen a gran profundidad dentro de la corteza terrestre, un error de +/- 1 ó 2 km es irrelevante; en otras palabras, se trata de un pequeño error cuando la profundidad es algo así como 13 km. Sin embargo, si la profundidad del seísmo es relativamente baja, el problema aumenta. Una profundidad negativa puede ser a veces un artefacto de la escasa resolución para un suceso poco profundo.\n",
    "\n",
    "En el caso de las explosiones de cantera registradas por la red sísmica, la profundidad se fija en 0 km, ya que nunca podemos determinar una profundidad precisa para ellas, pero sabemos que están muy cerca de la superficie.\n",
    "\n",
    "A veces, debido a la densidad de la red sísmica y a la proximidad de las estaciones sísmicas al epicentro de un seísmo, podemos determinar una profundidad muy precisa. Cuando la profundidad del seísmo es muy baja, puede notificarse como profundidad negativa.\n",
    "\n",
    "- https://www.usgs.gov/faqs/what-does-it-mean-earthquake-occurred-depth-0-km-how-can-earthquake-have-negative-depth-would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['properties.type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['properties.type'] != 'quarry blast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='properties.type', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un diccionario de mapeo de valores de alerta\n",
    "mapping = {\"green\": 1, \"yellow\": 2, \"orange\": 3, \"red\": 4}\n",
    "\n",
    "# Mapear los valores al DataFrame\n",
    "df[\"properties.alert\"] = df[\"properties.alert\"].map(mapping)\n",
    "df[\"properties.alert\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En sismología, la profundidad de foco, profundidad focal o profundidad del hipocentro se refiere a la profundidad a la que ocurre un terremoto. Los terremotos que ocurren a una profundidad de menos de 70 kilómetros (43,5 mi) se clasifican como terremotos de foco superficial, mientras que aquellos con una profundidad focal entre 70 kilómetros (43,5 mi) y 300 kilómetros (186,4 mi) se denominan comúnmente terremotos de foco medio o profundidad intermedia.1​ En las zonas de subducción, donde la corteza oceánica más antigua y fría desciende debajo de otra placa tectónica, pueden ocurrir terremotos de foco profundo a profundidades mucho mayores en el manto, que van desde 300 kilómetros (186,4 mi) hasta 700 kilómetros (435 mi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangos_profundida = {\n",
    "    'Superficial': (-4, 70),\n",
    "    'Media': (70, 300),\n",
    "    'Profunda': (300, 700)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"properties.gap\"].isna()]\n",
    "#capaz relacionado con otras fuentes que no sean earthquake, fijarse\n",
    "df[\"properties.gap\"].fillna(180, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape)\n",
    "df.dropna(subset='Profundidad', inplace=True)\n",
    "display(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copio el dataset para el modelo supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisado = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CategoriaProfundidad'] = pd.cut(df['Profundidad'], bins=[0, 70, 300, 700], labels=['Superficial', 'Media', 'Profunda'], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codificacion_profundidad = pd.get_dummies(df['CategoriaProfundidad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, codificacion_profundidad], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map={True: 1, False: 0}\n",
    "columnas_map = [\"Superficial\", \"Media\", \"Profunda\"]\n",
    "\n",
    "for columna in columnas_map:\n",
    "    df[columna] = df[columna].map(map)\n",
    "    df[columna]\n",
    "df.drop(columns='CategoriaProfundidad', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 7))\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot = True, cmap = plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **.corr()** aplica la correlación de Pearson. Los valores cercanos a cero demuestran una correlacion no lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_matrix(df, figsize=(12, 8))\n",
    "# # Rotar etiquetas en el eje x\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# # Rotar etiquetas en el eje y\n",
    "# plt.yticks(rotation=90)\n",
    "# plt.savefig('../MACHINE_LEARNING/scatter_matrix.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porcentajes = [(80, 20), (85, 15), (90, 10), (95, 5)]\n",
    "porcentaje = (85, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for porcentaje in porcentajes:\n",
    "# Obtener los porcentajes de entrenamiento y prueba\n",
    "porcentaje_entrenamiento, porcentaje_prueba = porcentaje\n",
    "\n",
    "# Calcular el tamaño del conjunto de prueba\n",
    "porcentaje_prueba = porcentaje_prueba / 100.0\n",
    "\n",
    "# Dividir el dataframe en conjuntos de entrenamiento y prueba\n",
    "X_entrenamiento, X_prueba = train_test_split(df, test_size=porcentaje_prueba, random_state=42)\n",
    "X_entrenamiento = preprocessing.Normalizer().fit_transform(X_entrenamiento)\n",
    "X_prueba = preprocessing.Normalizer().fit_transform(X_prueba)\n",
    "lista = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    algoritmo = KMeans(n_clusters = i, init = 'k-means++',\n",
    "                        max_iter = 300, n_init = 10)\n",
    "    algoritmo.fit(X_entrenamiento)\n",
    "    lista.append(algoritmo.inertia_)\n",
    "\n",
    "plt.figure(figsize = [10, 6])\n",
    "plt.title('Método de Codo')\n",
    "plt.xlabel('Nro de clusters')\n",
    "plt.ylabel('Inercia')\n",
    "plt.plot(list(range(1, 11)), lista, marker = 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmo = KMeans(n_clusters = 2, init = 'k-means++', \n",
    "                   max_iter = 300, n_init = 10)\n",
    "\n",
    "algoritmo.fit(X_entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroides, etiquetas = algoritmo.cluster_centers_, algoritmo.labels_\n",
    "\n",
    "muestras_prediccion = algoritmo.predict(X_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pred in enumerate(muestras_prediccion):\n",
    "  print('Muestra', i, ' Clúster:', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_pca = PCA(n_components = 2)\n",
    "modelo_pca.fit(X_entrenamiento)\n",
    "pca = modelo_pca.transform(X_entrenamiento) \n",
    "centroides_pca = modelo_pca.transform(centroides)\n",
    "display(f'Dimensiones de dataset de entrenamiento {X_entrenamiento.shape}')\n",
    "display(f'Dimensiones de dataset transformado (PCA):{pca.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.axes().set_aspect(\"equal\")\n",
    "plt.scatter(pca[:, 0], [2]*pca[:, 0].size, color='red', alpha=0.5, label='C1')\n",
    "plt.scatter(pca[:, 1], [1]*pca[:, 1].size, color='blue', alpha=0.5, label='C2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "display(f'Varianza explicada por cada componente {modelo_pca.explained_variance_}')\n",
    "display(f'Proporción de varianza explicada por cada componente {modelo_pca.explained_variance_ratio_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados\n",
    "plt.scatter(pca[:, 0], pca[:, 1])\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.title('PCA en 2 Dimensiones')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colores = ['red', 'green']\n",
    "\n",
    "colores_cluster = [colores[etiquetas[i]] for i in range(len(pca))]\n",
    "\n",
    "plt.scatter(pca[:, 0], pca[:, 1], c = colores_cluster, \n",
    "            marker = 'o',alpha = 0.4)\n",
    "\n",
    "plt.scatter(centroides_pca[:, 0], centroides_pca[:, 1],\n",
    "            marker = 'x', s = 100, linewidths = 3, c = colores)\n",
    "\n",
    "xvector = modelo_pca.components_[0] * max(pca[:,0])\n",
    "yvector = modelo_pca.components_[1] * max(pca[:,1])\n",
    "columnas = df.columns\n",
    "\n",
    "for i in range(len(columnas)):\n",
    "    #Se grafican los vectores\n",
    "    plt.arrow(0, 0, xvector[i], yvector[i], color = 'black', \n",
    "              width = 0.0005, head_width = 0.02, alpha = 0.75)\n",
    "    plt.text(xvector[i], yvector[i], list(columnas)[i], color='black', \n",
    "             alpha=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificacion supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangos_profundida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probabilidad de peligrosidad = mapa peligrosidad *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangos = {(-4, 70): 1, (70, 700): 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisado['Peligrosidad'] = df_supervisado['Profundidad'].apply(lambda x: next((value for key, value in rangos.items() if key[0] < x <= key[1]), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervisado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_x = ['properties.mag',\t'Profundidad']\n",
    "columnas_y = ['Peligrosidad']\n",
    "\n",
    "X = df_supervisado.loc[:, columnas_x]\n",
    "y = np.ravel(df_supervisado.loc[:, columnas_y])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "svm_rbf = SVC(kernel='rbf', random_state= 0)\n",
    "svm_mo = svm_rbf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediccion\n",
    "y_pred_svm = svm_rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_svm)\n",
    "print(cm)\n",
    "accuracy_score(y_test,y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, svm_rbf.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                color = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('SVM (Test set)')\n",
    "plt.xlabel('Magnitud')\n",
    "plt.ylabel('Profundidad')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combinaciones de dimensiones para graficar\n",
    "# combinations = [(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
    "\n",
    "# # Configuración de la grilla para los subplots\n",
    "# grid_rows = 5\n",
    "# grid_cols = 2\n",
    "\n",
    "# fig, axs = plt.subplots(grid_rows, grid_cols, figsize=(12, 15))\n",
    "\n",
    "# # Iterar sobre las combinaciones de dimensiones\n",
    "# for i, combination in enumerate(combinations):\n",
    "#     dim1, dim2 = combination\n",
    "    \n",
    "#     # Subplot correspondiente\n",
    "#     ax = axs[i // grid_cols, i % grid_cols]\n",
    "    \n",
    "#     X_set = X_test[:, [dim1, dim2]]\n",
    "#     y_set = y_test\n",
    "\n",
    "#     # Generar la cuadrícula para graficar\n",
    "#     X1, X2, X3, X4, X5 = np.meshgrid(\n",
    "#         np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n",
    "#         np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01),\n",
    "#         np.arange(start=X_set[:, 2].min() - 1, stop=X_set[:, 2].max() + 1, step=0.01),\n",
    "#         np.arange(start=X_set[:, 3].min() - 1, stop=X_set[:, 3].max() + 1, step=0.01),\n",
    "#         np.arange(start=X_set[:, 4].min() - 1, stop=X_set[:, 4].max() + 1, step=0.01)\n",
    "#     )\n",
    "\n",
    "#     # Preparar los datos para la clasificación y graficación\n",
    "#     X_grid = np.array([X1.ravel(), X2.ravel(), X3.ravel(), X4.ravel(), X5.ravel()]).T\n",
    "#     y_grid = svm_rbf.predict(X_grid)\n",
    "#     y_grid = y_grid.reshape(X1.shape)\n",
    "\n",
    "#     # Graficar los límites de decisión y los puntos de datos\n",
    "#     ax.contourf(X1, X2, y_grid, alpha=0.75, cmap=ListedColormap(('red', 'green')))\n",
    "#     ax.set_xlim(X1.min(), X1.max())\n",
    "#     ax.set_ylim(X2.min(), X2.max())\n",
    "\n",
    "#     # Graficar los puntos de datos\n",
    "#     for j, label in enumerate(np.unique(y_set)):\n",
    "#         ax.scatter(X_set[y_set == label, 0], X_set[y_set == label, 1],\n",
    "#                    c=ListedColormap(('red', 'green'))(j), label=label)\n",
    "\n",
    "#     # Configurar el título y etiquetas de los ejes\n",
    "#     ax.set_title(f\"Dimensions {dim1+1} and {dim2+1}\")\n",
    "#     ax.set_xlabel(f\"Feature {dim1+1}\")\n",
    "#     ax.set_ylabel(f\"Feature {dim2+1}\")\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector machine with poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los parámetros a explorar\n",
    "parameters = {'kernel': ['poly'], 'degree': [2, 3, 4, 5]}\n",
    "\n",
    "# Crear el modelo SVM\n",
    "svm_poly = SVC()\n",
    "\n",
    "# Realizar la búsqueda de grado utilizando validación cruzada\n",
    "grid_search = GridSearchCV(svm_poly, parameters, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Obtener el mejor grado y el modelo con el mejor desempeño\n",
    "best_degree = grid_search.best_params_['degree']\n",
    "best_model = grid_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fa81080852938d8f5a1a629d539a3c489c8a238a7314e7af8a2e2c28b6f2b60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
